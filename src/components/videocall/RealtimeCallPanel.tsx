import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { 
  Phone, PhoneOff, Mic, MicOff, Send, MessageSquare, 
  Video, VideoOff, Volume2, VolumeX, Camera
} from 'lucide-react';
import { useSimpleVoiceCall } from '@/hooks/useSimpleVoiceCall';
import { useSettings } from '@/hooks/useSettings';
import { toast } from 'sonner';
import { cn } from '@/lib/utils';
// supabase import removed - using fetch directly for vision-chat
import '@/styles/realtime-responsive.css';
import { Message } from '@/types/chat';

export type CallMode = 'voice' | 'video';

interface RealtimeCallPanelProps {
  onSpeakingChange?: (isSpeaking: boolean) => void;
  onCallStateChange?: (isInCall: boolean) => void;
  onAudioResponse?: (audioBase64: string) => void; // æ–°å¢ï¼šä¼ é€’éŸ³é¢‘ç»™ preset åŠ¨ç”»
}

export const RealtimeCallPanel: React.FC<RealtimeCallPanelProps> = ({
  onSpeakingChange,
  onCallStateChange,
  onAudioResponse
}) => {
  const { settings } = useSettings();
  const [inputText, setInputText] = useState('');
  const [showTextInput, setShowTextInput] = useState(false);
  const [callMode, setCallMode] = useState<CallMode>('voice');
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(true);
  const [callDuration, setCallDuration] = useState(0);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const frameIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const callTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const callStartTimeRef = useRef<Date | null>(null);
  const lastVisionContextRef = useRef<string>(''); // AIè§†è§‰ä¸Šä¸‹æ–‡
  const lastActivityRef = useRef<number>(Date.now()); // æ²‰é»˜æ£€æµ‹ï¼šæœ€åæ´»åŠ¨æ—¶é—´
  const silenceTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);

  // Build system prompt from settings + vision context
  const buildSystemPrompt = useCallback(() => {
    let prompt = `ä½ æ˜¯${settings.character.name}ã€‚
${settings.character.persona}
${settings.character.background}
è¯´è¯é£æ ¼ï¼š${settings.character.speakingStyle}
è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒå›ç­”ç®€æ´è‡ªç„¶ï¼ŒåƒçœŸäººå¯¹è¯ä¸€æ ·ã€‚`;

    if (callMode === 'video' && lastVisionContextRef.current) {
      prompt += `\n\n[è§†è§‰ä¸Šä¸‹æ–‡] ä½ å¯ä»¥çœ‹åˆ°ç”¨æˆ·çš„æ‘„åƒå¤´ç”»é¢ï¼š${lastVisionContextRef.current}`;
    }
    return prompt;
  }, [settings.character, callMode]);

  // ä½¿ç”¨ç®€åŒ–çš„è¯­éŸ³é€šè¯ hook (Web Speech API + sherpa-onnx fallback + MiniMax TTS)
  const {
    messages,
    isLoading,
    isConnected,
    isRecording,
    isPlaying: isSpeaking,
    isSpeechSupported,
    interimTranscript: transcript,
    connect,
    disconnect,
    startRecording,
    stopRecording,
    sendTextMessage,
    clearMessages,
    sttBackendActive,
    isSherpaModelLoading,
    sherpaLoadingStatus,
  } = useSimpleVoiceCall({
    settings,
    systemPrompt: buildSystemPrompt(),
    onSpeakingChange,
    onAudioResponse,
  });

  // æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´ï¼ˆç”¨æˆ·è¯´è¯æˆ–AIå›å¤æ—¶ï¼‰
  useEffect(() => {
    if (messages.length > 0) {
      const lastMsg = messages[messages.length - 1];
      if (lastMsg.role === 'user' || lastMsg.role === 'assistant') {
        lastActivityRef.current = Date.now();
      }
    }
  }, [messages]);

  // æ²‰é»˜æ£€æµ‹ï¼š8ç§’æ— æ´»åŠ¨æ—¶AIä¸»åŠ¨è¯´è¯
  useEffect(() => {
    if (!isConnected) {
      if (silenceTimerRef.current) {
        clearInterval(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      return;
    }

    // è¿æ¥æ—¶é‡ç½®æ´»åŠ¨æ—¶é—´
    lastActivityRef.current = Date.now();

    silenceTimerRef.current = setInterval(async () => {
      const elapsed = Date.now() - lastActivityRef.current;
      if (elapsed < 8000) return;
      if (isLoading || isSpeaking) return;

      if (callMode === 'video') {
        await captureAndAnalyzeFrame();
        sendTextMessage('[ç³»ç»Ÿ: ç”¨æˆ·å·²ç»æ²‰é»˜äº†ä¸€ä¼šå„¿ã€‚è¯·æ ¹æ®å½“å‰è§†è§‰ç”»é¢æˆ–ä¹‹å‰çš„å¯¹è¯ä¸»åŠ¨è¯´äº›ä»€ä¹ˆï¼Œæ¯”å¦‚è¯„è®ºçœ‹åˆ°çš„ç”»é¢ã€é—®ä¸€ä¸ªé—®é¢˜ã€æˆ–è¡¨è¾¾å…³å¿ƒã€‚ä¸è¦æåŠè¿™æ˜¯ç³»ç»ŸæŒ‡ä»¤ã€‚]');
      } else {
        sendTextMessage('[ç³»ç»Ÿ: ç”¨æˆ·å·²ç»æ²‰é»˜äº†ä¸€ä¼šå„¿ã€‚è¯·ä¸»åŠ¨è¯´äº›ä»€ä¹ˆæ¥ä¿æŒå¯¹è¯ï¼Œæ¯”å¦‚é—®ä¸€ä¸ªé—®é¢˜ã€åˆ†äº«ä¸€ä¸ªæƒ³æ³•ã€æˆ–è¡¨è¾¾å…³å¿ƒã€‚ä¸è¦æåŠè¿™æ˜¯ç³»ç»ŸæŒ‡ä»¤ã€‚]');
      }
      lastActivityRef.current = Date.now();
    }, 1000);

    return () => {
      if (silenceTimerRef.current) {
        clearInterval(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
    };
  }, [isConnected, isLoading, isSpeaking, callMode, captureAndAnalyzeFrame, sendTextMessage]);

  // Auto-scroll messages
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  // Notify call state changes
  useEffect(() => {
    onCallStateChange?.(isConnected);
  }, [isConnected, onCallStateChange]);

  // é€šè¯è®¡æ—¶å™¨
  useEffect(() => {
    if (isConnected) {
      callStartTimeRef.current = new Date();
      callTimerRef.current = setInterval(() => {
        if (callStartTimeRef.current) {
          const elapsed = Math.floor((Date.now() - callStartTimeRef.current.getTime()) / 1000);
          setCallDuration(elapsed);
        }
      }, 1000);
    } else {
      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
        callTimerRef.current = null;
      }
      setCallDuration(0);
      callStartTimeRef.current = null;
    }

    return () => {
      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
      }
    };
  }, [isConnected]);

  // æ ¼å¼åŒ–é€šè¯æ—¶é•¿
  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  // Start/stop camera based on mode
  const startCamera = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'user', width: 640, height: 480 },
        audio: false
      });
      streamRef.current = stream;
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        // ç¡®ä¿è§†é¢‘å¼€å§‹æ’­æ”¾
        try {
          await videoRef.current.play();
        } catch (e) {
          console.log('Video autoplay blocked, user interaction needed');
        }
      }
      return true;
    } catch (error) {
      console.error('Failed to start camera:', error);
      toast.error('æ— æ³•è®¿é—®æ‘„åƒå¤´');
      return false;
    }
  }, []);

  const stopCamera = useCallback(() => {
    if (frameIntervalRef.current) {
      clearInterval(frameIntervalRef.current);
      frameIntervalRef.current = null;
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  }, []);

  // å‘¨æœŸæ€§é‡‡æ ·æ‘„åƒå¤´å¸§å¹¶å‘é€ç»™AIåˆ†æ
  const captureAndAnalyzeFrame = useCallback(async () => {
    if (!videoRef.current || !canvasRef.current || callMode !== 'video') return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    if (!ctx || video.readyState < 2) return;

    // è®¾ç½®canvaså°ºå¯¸
    canvas.width = 320; // é™ä½åˆ†è¾¨ç‡èŠ‚çœä¼ è¾“
    canvas.height = 240;

    // ç»˜åˆ¶å¹¶å¯¼å‡º
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const imageDataUrl = canvas.toDataURL('image/jpeg', 0.6);

    try {
      // ç›´æ¥ç”¨ fetch è°ƒç”¨ vision-chat å¹¶è§£æ SSE æµ
      const response = await fetch(
        `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/vision-chat`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY}`,
          },
          body: JSON.stringify({
            messages: [{ role: 'user', content: 'è¯·ç”¨ä¸€å¥è¯ç®€æ´æè¿°ä½ çœ‹åˆ°çš„ç”»é¢ã€‚' }],
            systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªè§†è§‰åˆ†æåŠ©æ‰‹ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡æè¿°ç”»é¢å†…å®¹ã€‚',
            image: imageDataUrl,
          }),
        }
      );

      if (!response.ok) {
        console.error('[Vision] API error:', response.status);
        return;
      }

      // è§£æ SSE æµæå–æ–‡æœ¬å†…å®¹
      const reader = response.body?.getReader();
      if (!reader) return;

      const decoder = new TextDecoder();
      let visionText = '';
      let textBuffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        textBuffer += decoder.decode(value, { stream: true });

        let newlineIndex: number;
        while ((newlineIndex = textBuffer.indexOf('\n')) !== -1) {
          let line = textBuffer.slice(0, newlineIndex);
          textBuffer = textBuffer.slice(newlineIndex + 1);

          if (line.endsWith('\r')) line = line.slice(0, -1);
          if (!line.startsWith('data: ')) continue;

          const jsonStr = line.slice(6).trim();
          if (jsonStr === '[DONE]') break;

          try {
            const parsed = JSON.parse(jsonStr);
            const delta = parsed.choices?.[0]?.delta?.content;
            if (delta) visionText += delta;
          } catch {
            // ä¸å®Œæ•´çš„ JSONï¼Œå¿½ç•¥
          }
        }
      }

      if (visionText) {
        lastVisionContextRef.current = visionText.slice(0, 200);
        console.log('[Vision] Context updated:', lastVisionContextRef.current);
      }
    } catch (e) {
      console.error('[Vision] Frame analysis error:', e);
    }
  }, [callMode]);

  // å¼€å§‹å‘¨æœŸæ€§é‡‡æ ·ï¼ˆè§†é¢‘æ¨¡å¼ï¼‰
  const startFrameCapture = useCallback(() => {
    if (frameIntervalRef.current) {
      clearInterval(frameIntervalRef.current);
    }
    // æ¯3ç§’é‡‡æ ·ä¸€æ¬¡
    frameIntervalRef.current = setInterval(captureAndAnalyzeFrame, 3000);
    // ç«‹å³é‡‡æ ·ä¸€æ¬¡
    setTimeout(captureAndAnalyzeFrame, 500);
  }, [captureAndAnalyzeFrame]);

  // åˆ‡æ¢é€šè¯æ¨¡å¼
  const handleModeSwitch = async () => {
    if (!isConnected) {
      setCallMode(prev => prev === 'voice' ? 'video' : 'voice');
      return;
    }

    const newMode = callMode === 'voice' ? 'video' : 'voice';
    
    if (newMode === 'video') {
      const success = await startCamera();
      if (success) {
        setCallMode('video');
        startFrameCapture();
        toast.info('å·²åˆ‡æ¢åˆ°è§†é¢‘é€šè¯ï¼Œæ³¨æ„éšç§ä¿æŠ¤');
      }
    } else {
      stopCamera();
      setCallMode('voice');
      lastVisionContextRef.current = '';
    }
  };

  // é™éŸ³æ§åˆ¶
  const handleMuteToggle = () => {
    if (isMuted) {
      startRecording();
      setIsMuted(false);
    } else {
      stopRecording();
      setIsMuted(true);
    }
  };

  const handleStartCall = async () => {
    // æ£€æŸ¥è¯­éŸ³è®¾ç½®
    if (!settings.voiceConfig.enabled) {
      toast.error('è¯·å…ˆåœ¨è®¾ç½®ä¸­å¯ç”¨è¯­éŸ³åŠŸèƒ½');
      return;
    }
    if (!settings.voiceConfig.minimaxApiKey || !settings.voiceConfig.minimaxGroupId) {
      toast.error('è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® MiniMax API');
      return;
    }

    if (callMode === 'video') {
      const cameraSuccess = await startCamera();
      if (!cameraSuccess) {
        // é™çº§åˆ°è¯­éŸ³æ¨¡å¼
        setCallMode('voice');
      }
    }
    
    const success = await connect();
    if (!success) {
      toast.error('æ— æ³•å¯åŠ¨è¯­éŸ³è¯†åˆ«');
      return;
    }
    
    toast.success('é€šè¯å·²è¿æ¥ï¼Œå¼€å§‹è¯´è¯å§ï¼');
    
    // è¿æ¥åè‡ªåŠ¨å¼€å§‹å½•éŸ³
    setTimeout(() => {
      startRecording();
      if (callMode === 'video' && streamRef.current) {
        startFrameCapture();
      }
    }, 500);
  };

  const handleEndCall = () => {
    stopRecording();
    disconnect();
    stopCamera();
    lastVisionContextRef.current = '';
    if (silenceTimerRef.current) {
      clearInterval(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    clearMessages();
  };

  const handleSendText = () => {
    if (!inputText.trim() || !isConnected) return;
    sendTextMessage(inputText);
    setInputText('');
    setShowTextInput(false);
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSendText();
    }
  };

  return (
    <div className="realtime-call-panel h-full flex flex-col bg-gradient-to-b from-background to-muted/20 relative">
      {/* éšè—çš„canvasç”¨äºå¸§é‡‡æ · */}
      <canvas ref={canvasRef} className="hidden" />

      {/* å¾®ä¿¡é£æ ¼é¡¶éƒ¨çŠ¶æ€æ  */}
      {isConnected && (
        <div className="absolute top-0 left-0 right-0 z-20 bg-gradient-to-b from-background/90 to-transparent p-3">
          <div className="flex items-center justify-between">
            <div className="flex items-center gap-2">
              <span className="text-sm font-medium">{settings.character.name}</span>
              <span className={cn(
                "px-2 py-0.5 rounded-full text-xs",
                callMode === 'video' 
                  ? "bg-primary/20 text-primary" 
                  : "bg-muted text-muted-foreground"
              )}>
                {callMode === 'video' ? 'è§†é¢‘é€šè¯' : 'è¯­éŸ³é€šè¯'}
              </span>
            </div>
            <div className="flex items-center gap-2 text-sm text-muted-foreground">
              <span>{formatDuration(callDuration)}</span>
              <div className="w-2 h-2 bg-emerald-500 rounded-full animate-pulse" />
            </div>
          </div>
        </div>
      )}

      {/* æ¶ˆæ¯åŒºåŸŸ */}
      <div className="flex-1 overflow-y-auto p-4 pt-14 space-y-3">
        {/* sherpa-onnx æ¨¡å‹åŠ è½½çŠ¶æ€æç¤º */}
        {isSherpaModelLoading && (
          <div className="bg-blue-500/10 border border-blue-500/30 rounded-lg p-4 mb-3">
            <p className="text-sm text-blue-600">
              {sherpaLoadingStatus || 'æ­£åœ¨åŠ è½½ç¦»çº¿è¯­éŸ³è¯†åˆ«æ¨¡å‹...'}
            </p>
            <p className="text-xs text-blue-500 mt-1">
              é¦–æ¬¡ä½¿ç”¨éœ€ä¸‹è½½çº¦200MBæ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
            </p>
          </div>
        )}

        {/* STT åç«¯æŒ‡ç¤º */}
        {isConnected && sttBackendActive === 'sherpa-onnx' && !isSherpaModelLoading && (
          <div className="bg-emerald-500/10 border border-emerald-500/30 rounded-lg p-3 mb-3">
            <p className="text-xs text-emerald-600">
              ğŸ”§ ä½¿ç”¨ç¦»çº¿è¯­éŸ³è¯†åˆ« (sherpa-onnx)
            </p>
          </div>
        )}

        {/* æµè§ˆå™¨ä¸æ”¯æŒæç¤º */}
        {!isSpeechSupported && (
          <div className="bg-amber-500/10 border border-amber-500/30 rounded-lg p-4 mb-3">
            <p className="text-sm text-amber-600">
              è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ã€‚
            </p>
          </div>
        )}

        {messages.length === 0 && !isConnected && (
          <div className="flex flex-col items-center justify-center h-full text-muted-foreground">
            <Phone className="h-12 w-12 mb-4 opacity-50" />
            <p className="text-center">ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹{callMode === 'video' ? 'è§†é¢‘' : 'è¯­éŸ³'}é€šè¯</p>
            <p className="text-sm mt-2 opacity-70">
              {callMode === 'video' 
                ? 'è§†é¢‘æ¨¡å¼ä¸‹ AI å¯ä»¥çœ‹è§ä½ çš„ç”»é¢' 
                : 'è¯­éŸ³æ¨¡å¼ï¼Œæ”¯æŒå®æ—¶å¯¹è¯'}
            </p>
          </div>
        )}
        
        {messages.map((msg) => (
          <div
            key={msg.id}
            className={cn(
              "max-w-[85%] p-3 rounded-2xl",
              msg.role === 'user'
                ? "ml-auto bg-primary text-primary-foreground rounded-br-md"
                : "mr-auto bg-muted rounded-bl-md"
            )}
          >
            <p className="text-sm whitespace-pre-wrap">{msg.content}</p>
          </div>
        ))}
        
        {/* å®æ—¶è½¬å†™æŒ‡ç¤ºå™¨ */}
        {isRecording && !isMuted && transcript && (
          <div className="max-w-[85%] ml-auto p-3 rounded-2xl bg-primary/50 text-primary-foreground rounded-br-md animate-pulse">
            <p className="text-sm">{transcript}...</p>
          </div>
        )}
        
        <div ref={messagesEndRef} />
      </div>

      {/* ç”¨æˆ·æ‘„åƒå¤´ PiPï¼ˆä»…è§†é¢‘æ¨¡å¼ï¼‰ */}
      {isConnected && callMode === 'video' && (
        <div className="absolute top-14 right-4 w-28 h-36 rounded-xl overflow-hidden shadow-lg border-2 border-primary/30 z-10 bg-black">
          <video
            ref={videoRef}
            autoPlay
            playsInline
            muted
            className="w-full h-full object-cover transform scale-x-[-1]"
          />
          {/* å½•éŸ³æŒ‡ç¤ºå™¨ */}
          {isRecording && !isMuted && (
            <div className="absolute top-2 right-2 w-3 h-3 bg-destructive rounded-full animate-pulse" />
          )}
          {/* é™éŸ³æŒ‡ç¤ºå™¨ */}
          {isMuted && (
            <div className="absolute bottom-2 left-1/2 -translate-x-1/2 bg-destructive/80 rounded-full p-1">
              <MicOff className="h-3 w-3 text-white" />
            </div>
          )}
        </div>
      )}

      {/* æ–‡å­—è¾“å…¥ï¼ˆå¯é€‰ï¼‰ */}
      {showTextInput && isConnected && (
        <div className="p-4 border-t flex gap-2">
          <Textarea
            value={inputText}
            onChange={(e) => setInputText(e.target.value)}
            onKeyDown={handleKeyDown}
            placeholder="è¾“å…¥æ¶ˆæ¯..."
            className="min-h-[40px] max-h-[100px] resize-none"
            rows={1}
          />
          <Button onClick={handleSendText} size="icon" disabled={!inputText.trim()}>
            <Send className="h-4 w-4" />
          </Button>
        </div>
      )}

      {/* å¾®ä¿¡é£æ ¼åº•éƒ¨æ§åˆ¶æ  */}
      <div className="p-4 border-t bg-background/80 backdrop-blur">
        {!isConnected ? (
          /* æœªæ¥é€šçŠ¶æ€ */
          <div className="flex flex-col items-center gap-4">
            {/* æ¨¡å¼åˆ‡æ¢ */}
            <div className="flex items-center gap-2 bg-muted rounded-full p-1">
              <button
                onClick={() => setCallMode('voice')}
                className={cn(
                  "px-4 py-2 rounded-full text-sm transition-colors",
                  callMode === 'voice' 
                    ? "bg-primary text-primary-foreground" 
                    : "text-muted-foreground hover:text-foreground"
                )}
              >
                <Volume2 className="h-4 w-4 inline mr-1" />
                è¯­éŸ³
              </button>
              <button
                onClick={() => setCallMode('video')}
                className={cn(
                  "px-4 py-2 rounded-full text-sm transition-colors",
                  callMode === 'video' 
                    ? "bg-primary text-primary-foreground" 
                    : "text-muted-foreground hover:text-foreground"
                )}
              >
                <Video className="h-4 w-4 inline mr-1" />
                è§†é¢‘
              </button>
            </div>
            
            {/* å¼€å§‹é€šè¯æŒ‰é’® */}
            <Button
              onClick={handleStartCall}
              size="lg"
              className="rounded-full h-16 w-16 bg-emerald-600 hover:bg-emerald-700"
            >
              {callMode === 'video' ? <Video className="h-6 w-6" /> : <Phone className="h-6 w-6" />}
            </Button>
          </div>
        ) : (
          /* é€šè¯ä¸­æ§åˆ¶æ  */
          <div className="flex items-center justify-center gap-3">
            {/* é™éŸ³ */}
            <Button
              onClick={handleMuteToggle}
              size="icon"
              variant={isMuted ? "destructive" : "outline"}
              className="rounded-full h-12 w-12"
            >
              {isMuted ? <MicOff className="h-5 w-5" /> : <Mic className="h-5 w-5" />}
            </Button>

            {/* åˆ‡æ¢è¯­éŸ³/è§†é¢‘ */}
            <Button
              onClick={handleModeSwitch}
              size="icon"
              variant="outline"
              className="rounded-full h-12 w-12"
            >
              {callMode === 'video' ? <VideoOff className="h-5 w-5" /> : <Video className="h-5 w-5" />}
            </Button>

            {/* æ‰¬å£°å™¨ */}
            <Button
              onClick={() => setIsSpeakerOn(!isSpeakerOn)}
              size="icon"
              variant={isSpeakerOn ? "outline" : "secondary"}
              className="rounded-full h-12 w-12"
            >
              {isSpeakerOn ? <Volume2 className="h-5 w-5" /> : <VolumeX className="h-5 w-5" />}
            </Button>

            {/* æ–‡å­—æ¶ˆæ¯ */}
            <Button
              onClick={() => setShowTextInput(!showTextInput)}
              size="icon"
              variant={showTextInput ? "default" : "outline"}
              className="rounded-full h-12 w-12"
            >
              <MessageSquare className="h-5 w-5" />
            </Button>

            {/* æŒ‚æ–­ */}
            <Button
              onClick={handleEndCall}
              size="lg"
              className="rounded-full h-14 w-14 bg-destructive hover:bg-destructive/90"
            >
              <PhoneOff className="h-6 w-6" />
            </Button>
          </div>
        )}

        {/* çŠ¶æ€æŒ‡ç¤ºå™¨ */}
        {isConnected && (
          <div className="flex items-center justify-center gap-4 mt-3 text-sm text-muted-foreground">
            {isRecording && !isMuted && (
              <span className="flex items-center gap-1">
                <span className="w-2 h-2 bg-emerald-500 rounded-full animate-pulse" />
                æ­£åœ¨è†å¬...
              </span>
            )}
            {isSpeaking && (
              <span className="flex items-center gap-1">
                <span className="w-2 h-2 bg-primary rounded-full animate-pulse" />
                AI æ­£åœ¨è¯´è¯...
              </span>
            )}
            {callMode === 'video' && (
              <span className="flex items-center gap-1 text-primary/70">
                <Camera className="w-3 h-3" />
                AIå¯è§
              </span>
            )}
          </div>
        )}
      </div>
    </div>
  );
};
