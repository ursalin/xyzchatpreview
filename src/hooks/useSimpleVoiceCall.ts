import { useState, useCallback, useRef, useEffect } from 'react';
import { Message, AppSettings, defaultVoiceConfig } from '@/types/chat';
import { useWebSpeechSTT } from './useWebSpeechSTT';
import { useXunfeiSTT } from './useXunfeiSTT';
import { removeParenthesesContent } from '@/lib/textUtils';

interface UseSimpleVoiceCallOptions {
  settings: AppSettings;
  systemPrompt: string;
  onSpeakingChange?: (isSpeaking: boolean) => void;
  onAudioResponse?: (audioBase64: string) => void;
}

export function useSimpleVoiceCall({
  settings,
  systemPrompt,
  onSpeakingChange,
  onAudioResponse,
}: UseSimpleVoiceCallOptions) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  
  const audioRef = useRef<HTMLAudioElement | null>(null);
  // ç”¨ ref ä¿å­˜æœ€æ–°çš„ messagesï¼Œé¿å…é—­åŒ…è¿‡æœŸ
  const messagesRef = useRef<Message[]>([]);
  const isConnectedRef = useRef(false);
  const systemPromptRef = useRef(systemPrompt);
  // STT æš‚åœ/æ¢å¤æ§åˆ¶ï¼Œåœ¨ TTS æ’­æ”¾æ—¶æš‚åœï¼Œé˜²æ­¢å›å£°
  const pauseSTTRef = useRef<(() => void) | null>(null);
  const resumeSTTRef = useRef<(() => void) | null>(null);
  // TTS æ’­æ”¾ä¸­æ ‡å¿—ï¼štrue æ—¶å¿½ç•¥æ‰€æœ‰ STT ç»“æœï¼ˆé˜²æ­¢ç¼“å†²çš„è¯†åˆ«ç»“æœè§¦å‘å‘é€ï¼‰
  const isTTSPlayingRef = useRef(false);

  // åŒæ­¥ state åˆ° ref
  useEffect(() => {
    messagesRef.current = messages;
  }, [messages]);

  useEffect(() => {
    isConnectedRef.current = isConnected;
  }, [isConnected]);

  useEffect(() => {
    systemPromptRef.current = systemPrompt;
  }, [systemPrompt]);

  // TTS æ’­æ”¾ï¼ˆç”¨ useCallback ä½†é€šè¿‡ ref è¯»å– settingsï¼‰
  const settingsRef = useRef(settings);
  useEffect(() => {
    settingsRef.current = settings;
  }, [settings]);

  const speak = useCallback(async (text: string) => {
    const { voiceConfig } = settingsRef.current;
    // Fallback to defaults if saved config has empty keys
    const apiKey = voiceConfig.minimaxApiKey || defaultVoiceConfig.minimaxApiKey;
    const groupId = voiceConfig.minimaxGroupId || defaultVoiceConfig.minimaxGroupId;
    const voice = voiceConfig.voiceId || defaultVoiceConfig.voiceId;
    const enabled = voiceConfig.minimaxApiKey ? voiceConfig.enabled : defaultVoiceConfig.enabled;

    if (!enabled || !apiKey || !groupId) {
      console.log('Voice not enabled or missing config, skipping TTS');
      return;
    }

    const textToSpeak = removeParenthesesContent(text);
    if (!textToSpeak) return;

    try {
      console.log('[TTS] Generating audio for:', textToSpeak.substring(0, 50) + '...');
      console.log('[TTS] Config:', { voiceId: voice, groupId: groupId.substring(0, 6) + '...' });
      
      // ç›´æ¥è°ƒMiniMax APIï¼Œä¸èµ°Supabase edge function
      const response = await fetch(
        `https://api.minimax.chat/v1/t2a_v2?GroupId=${groupId}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: 'speech-01-turbo',
            text: textToSpeak,
            stream: false,
            voice_setting: {
              voice_id: voice,
              speed: 1.0,
              vol: 1.0,
              pitch: 0,
            },
            audio_setting: {
              sample_rate: 32000,
              bitrate: 128000,
              format: 'mp3',
            },
          }),
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error('[TTS] API error:', response.status, errorText);
        import('sonner').then(({ toast }) => toast.error(`è¯­éŸ³åˆæˆå¤±è´¥: ${response.status}`));
        return;
      }

      const result = await response.json();
      
      if (result.base_resp?.status_code !== 0) {
        const errMsg = result.base_resp?.status_msg || 'MiniMax API error';
        console.error('[TTS] MiniMax error:', errMsg);
        import('sonner').then(({ toast }) => toast.error(`è¯­éŸ³åˆæˆå¤±è´¥: ${errMsg}`));
        return;
      }

      const audioHex = result.data?.audio;
      if (!audioHex) {
        console.error('[TTS] No audio data in response');
        import('sonner').then(({ toast }) => toast.error('è¯­éŸ³åˆæˆå¤±è´¥: æ— éŸ³é¢‘æ•°æ®'));
        return;
      }

      // Convert hex to base64
      const bytes = new Uint8Array(audioHex.length / 2);
      for (let i = 0; i < audioHex.length; i += 2) {
        bytes[i / 2] = parseInt(audioHex.substring(i, i + 2), 16);
      }
      let binary = '';
      for (let i = 0; i < bytes.length; i++) {
        binary += String.fromCharCode(bytes[i]);
      }
      const audioBase64 = btoa(binary);

      if (audioBase64) {
        console.log('[TTS] Got audio, length:', audioBase64.length);
        onAudioResponse?.(audioBase64);
        
        const audioUrl = `data:audio/mpeg;base64,${audioBase64}`;
        const audio = new Audio(audioUrl);
        audioRef.current = audio;
        
        // æ’­æ”¾å‰æš‚åœ STTï¼Œé˜²æ­¢è§’è‰²å£°éŸ³è¢«è¯†åˆ«æˆç”¨æˆ·è¾“å…¥
        isTTSPlayingRef.current = true;
        pauseSTTRef.current?.();

        audio.onplay = () => {
          console.log('[TTS] Audio playing');
          setIsPlaying(true);
        };
        audio.onended = () => {
          console.log('[TTS] Audio ended');
          setIsPlaying(false);
          // æ’­å®Œæ¢å¤ STTï¼ˆå»¶è¿Ÿæ¸…é™¤æ ‡å¿—ï¼Œç¡®ä¿æ®‹ä½™è¯†åˆ«ç»“æœè¢«ä¸¢å¼ƒï¼‰
          setTimeout(() => {
            isTTSPlayingRef.current = false;
            resumeSTTRef.current?.();
          }, 500);
        };
        audio.onerror = (e) => {
          console.error('[TTS] Audio playback error:', e);
          setIsPlaying(false);
          setTimeout(() => {
            isTTSPlayingRef.current = false;
            resumeSTTRef.current?.();
          }, 500);
          import('sonner').then(({ toast }) => toast.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥'));
        };
        
        await audio.play();
      } else {
        console.error('[TTS] No audio data in response');
        import('sonner').then(({ toast }) => toast.error('è¯­éŸ³åˆæˆè¿”å›ç©ºæ•°æ®'));
      }
    } catch (error) {
      console.error('[TTS] Error:', error);
      import('sonner').then(({ toast }) => toast.error(`è¯­éŸ³åˆæˆå¼‚å¸¸: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`));
    }
  }, [onAudioResponse]);

  // å‘é€æ¶ˆæ¯ç»™ AIï¼ˆä¸ä¾èµ– messages stateï¼Œç”¨ ref è¯»å–ï¼‰
  const sendMessageToAI = useCallback(async (content: string) => {
    if (isLoading) {
      console.log('Already loading, skipping duplicate send');
      return null;
    }

    const userMessage: Message = {
      id: crypto.randomUUID(),
      role: 'user',
      content,
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      // ç”¨ ref è¯»å–æœ€æ–° messages æ„å»ºä¸Šä¸‹æ–‡
      const currentMessages = messagesRef.current;
      
      // æ¯æ¬¡å‘æ¶ˆæ¯æ—¶å®æ—¶æ³¨å…¥å½“å‰æ—¶é—´
      const nowDate = new Date();
      const nowStr = nowDate.toLocaleString('zh-CN', {
        timeZone: 'Asia/Shanghai',
        year: 'numeric',
        month: 'long',
        day: 'numeric',
        weekday: 'long',
        hour: '2-digit',
        minute: '2-digit',
      });
      const cnHour = parseInt(nowDate.toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai', hour: 'numeric', hour12: false }));
      const period = cnHour < 6 ? 'å‡Œæ™¨' : cnHour < 9 ? 'æ—©ä¸Š' : cnHour < 12 ? 'ä¸Šåˆ' : cnHour < 14 ? 'ä¸­åˆ' : cnHour < 18 ? 'ä¸‹åˆ' : cnHour < 22 ? 'æ™šä¸Š' : 'æ·±å¤œ';
      const realtimePrompt = systemPromptRef.current.replace(
        /å½“å‰æ—¶é—´ï¼š.*/,
        `å½“å‰æ—¶é—´ï¼š${nowStr}ï¼ˆ${period}ï¼‰`
      );
      
      const response = await fetch(
        `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/chat`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY}`,
          },
          body: JSON.stringify({
            messages: [...currentMessages, userMessage].slice(-20).map(m => ({
              role: m.role,
              content: m.content,
            })),
            systemPrompt: realtimePrompt,
            maxTokens: 100,
          }),
        }
      );

      if (!response.ok) {
        const errorText = await response.text().catch(() => `HTTP ${response.status}`);
        console.error('Chat API error:', response.status, errorText);
        throw new Error(`AI å›å¤å¤±è´¥ (${response.status})`);
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('æ— æ³•è¯»å–å“åº”');

      const decoder = new TextDecoder();
      let assistantContent = '';
      const assistantMessageId = crypto.randomUUID();

      setMessages(prev => [
        ...prev,
        {
          id: assistantMessageId,
          role: 'assistant',
          content: '',
          timestamp: new Date(),
        },
      ]);

      let textBuffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        textBuffer += decoder.decode(value, { stream: true });

        let newlineIndex: number;
        while ((newlineIndex = textBuffer.indexOf('\n')) !== -1) {
          let line = textBuffer.slice(0, newlineIndex);
          textBuffer = textBuffer.slice(newlineIndex + 1);

          if (line.endsWith('\r')) line = line.slice(0, -1);
          if (line.startsWith(':') || line.trim() === '') continue;
          if (!line.startsWith('data: ')) continue;

          const jsonStr = line.slice(6).trim();
          if (jsonStr === '[DONE]') break;

          try {
            const parsed = JSON.parse(jsonStr);
            const deltaContent = parsed.choices?.[0]?.delta?.content as string | undefined;
            if (deltaContent) {
              assistantContent += deltaContent;
              setMessages(prev =>
                prev.map(m =>
                  m.id === assistantMessageId
                    ? { ...m, content: assistantContent }
                    : m
                )
              );
            }
          } catch {
            // ä¸å®Œæ•´çš„ JSONï¼Œæ”¾å› buffer
            textBuffer = line + '\n' + textBuffer;
            break;
          }
        }
      }

      // è‡ªåŠ¨æ’­æ”¾ TTS
      console.log('[TTS] Auto-play check:', { 
        hasContent: !!assistantContent, 
        contentLength: assistantContent?.length,
        enabled: settingsRef.current.voiceConfig.enabled,
        hasApiKey: !!settingsRef.current.voiceConfig.minimaxApiKey,
      });
      if (assistantContent && settingsRef.current.voiceConfig.enabled) {
        console.log('[TTS] Triggering speak for:', assistantContent.substring(0, 30) + '...');
        import('sonner').then(({ toast }) => toast.info(`ğŸ”Š TTSå¼€å§‹: "${assistantContent.substring(0, 20)}..."`));
        await speak(assistantContent);
      } else {
        console.warn('[TTS] Skipped! Content empty or voice disabled');
        import('sonner').then(({ toast }) => toast.warning(`âš ï¸ TTSè·³è¿‡: å†…å®¹=${!!assistantContent}, è¯­éŸ³å¼€å…³=${settingsRef.current.voiceConfig.enabled}, API Key=${!!settingsRef.current.voiceConfig.minimaxApiKey}`));
      }

      return assistantContent;
    } catch (error) {
      console.error('Chat error:', error);
      const errorMessage: Message = {
        id: crypto.randomUUID(),
        role: 'assistant',
        content: error instanceof Error ? error.message : 'æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, errorMessage]);
      return null;
    } finally {
      setIsLoading(false);
    }
  }, [isLoading, speak]);

  // ç”¨ ref ä¿å­˜ sendMessageToAIï¼Œè®© STT å›è°ƒå§‹ç»ˆè°ƒç”¨æœ€æ–°ç‰ˆæœ¬
  const sendMessageToAIRef = useRef(sendMessageToAI);
  useEffect(() => {
    sendMessageToAIRef.current = sendMessageToAI;
  }, [sendMessageToAI]);

  // STT å›è°ƒå‡½æ•°ï¼ˆWeb Speech å’Œè®¯é£å…±ç”¨ï¼‰
  const handleSTTResult = useCallback((transcript: string, isFinal: boolean) => {
    // TTS æ’­æ”¾ä¸­å¿½ç•¥æ‰€æœ‰è¯†åˆ«ç»“æœï¼Œé˜²æ­¢å›å£°
    if (isTTSPlayingRef.current) {
      console.log('[STT] Ignoring result during TTS playback:', transcript.substring(0, 30));
      return;
    }
    if (isFinal) {
      if (transcript.trim() && isConnectedRef.current) {
        console.log('[Voice] Sending final transcript:', transcript.trim());
        sendMessageToAIRef.current(transcript.trim());
      }
      setInterimTranscript('');
    } else {
      setInterimTranscript(transcript);
    }
  }, []);

  const handleSTTError = useCallback((error: string) => {
    console.error('[STT Error]', error);
    setInterimTranscript('');
  }, []);

  // å°è¯•ä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«
  const xunfeiSTT = useXunfeiSTT({
    language: 'zh_cn',
    onResult: handleSTTResult,
    onError: handleSTTError,
  });

  // å¤‡ç”¨ï¼šWeb Speech API
  const webSpeechSTT = useWebSpeechSTT({
    language: 'zh-CN',
    onResult: handleSTTResult,
    onError: handleSTTError,
  });

  // ä¼˜å…ˆä½¿ç”¨è®¯é£ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ Web Speech API
  const sttEngine = xunfeiSTT.isSupported ? xunfeiSTT : webSpeechSTT;
  const {
    isListening: isRecording,
    isSupported: isSpeechSupported,
    interimTranscript: sttInterim,
    startListening,
    stopListening,
  } = sttEngine;

  console.log('[STT] Using engine:', xunfeiSTT.isSupported ? 'Xunfei' : 'Web Speech API');

  // ç»‘å®š STT æš‚åœ/æ¢å¤åˆ° refï¼Œä¾› speak() ä½¿ç”¨
  useEffect(() => {
    pauseSTTRef.current = () => {
      console.log('[STT] Pausing for TTS playback');
      stopListening();
    };
    resumeSTTRef.current = () => {
      if (isConnectedRef.current) {
        console.log('[STT] Resuming after TTS playback');
        setTimeout(() => startListening(), 300);
      }
    };
  }, [stopListening, startListening]);

  // åŒæ­¥ä¸´æ—¶è¯†åˆ«ç»“æœ
  useEffect(() => {
    setInterimTranscript(sttInterim);
  }, [sttInterim]);

  // é€šçŸ¥çˆ¶ç»„ä»¶è¯´è¯çŠ¶æ€
  useEffect(() => {
    onSpeakingChange?.(isPlaying);
  }, [isPlaying, onSpeakingChange]);

  // åœæ­¢æ’­æ”¾
  const stopPlaying = useCallback(() => {
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
      setIsPlaying(false);
    }
  }, []);

  // å¼€å§‹é€šè¯
  const connect = useCallback(async () => {
    if (!isSpeechSupported) {
      console.error('Web Speech API not supported');
      return false;
    }
    setIsConnected(true);
    startListening();
    return true;
  }, [isSpeechSupported, startListening]);

  // ç»“æŸé€šè¯
  const disconnect = useCallback(() => {
    stopListening();
    stopPlaying();
    setIsConnected(false);
  }, [stopListening, stopPlaying]);

  // å¼€å§‹å½•éŸ³
  const startRecording = useCallback(() => {
    if (isConnectedRef.current) {
      startListening();
    }
  }, [startListening]);

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(() => {
    stopListening();
  }, [stopListening]);

  // å‘é€æ–‡å­—æ¶ˆæ¯
  const sendTextMessage = useCallback((text: string) => {
    if (text.trim() && isConnectedRef.current) {
      sendMessageToAIRef.current(text.trim());
    }
  }, []);

  // æ¸…ç©ºæ¶ˆæ¯
  const clearMessages = useCallback(() => {
    setMessages([]);
  }, []);

  return {
    messages,
    isLoading,
    isConnected,
    isRecording,
    isPlaying,
    isSpeechSupported,
    interimTranscript,
    connect,
    disconnect,
    startRecording,
    stopRecording,
    sendTextMessage,
    clearMessages,
    stopPlaying,
  };
}
