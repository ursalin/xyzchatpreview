import { useState, useCallback, useRef, useEffect } from 'react';
import { Message, AppSettings } from '@/types/chat';
import { supabase } from '@/integrations/supabase/client';
import { removeParenthesesContent } from '@/lib/textUtils';
import { useWebSpeechSTT } from './useWebSpeechSTT';
import { useXunfeiSTT } from './useXunfeiSTT';
import { useMemoryManager } from './useMemoryManager';

// è§’è‰²å›¾ç‰‡ URLï¼ˆéœ€è¦æ˜¯å…¬å¼€å¯è®¿é—®çš„ URLï¼‰
import characterFrontImg from '@/assets/character-front.jpg';

// å”‡å½¢åŠ¨ç”»è§†é¢‘ç¼“å­˜ï¼ˆåŸºäºæ–‡æœ¬å“ˆå¸Œï¼‰
interface LipsyncCacheEntry {
  videoUrl: string;
  audioBase64: string;
  createdAt: number;
}

const CACHE_EXPIRY_MS = 60 * 60 * 1000; // 1å°æ—¶è¿‡æœŸ
const CACHE_KEY = 'lipsync_video_cache';
const MAX_CACHE_ENTRIES = 20;
const CHAT_HISTORY_KEY = 'ai-companion-chat-history';
const MAX_STORED_MESSAGES = 100;

// ç®€å•çš„æ–‡æœ¬å“ˆå¸Œå‡½æ•°
function hashText(text: string): string {
  let hash = 0;
  for (let i = 0; i < text.length; i++) {
    const char = text.charCodeAt(i);
    hash = ((hash << 5) - hash) + char;
    hash = hash & hash;
  }
  return hash.toString(36);
}

// ä» localStorage åŠ è½½ç¼“å­˜
function loadCache(): Map<string, LipsyncCacheEntry> {
  try {
    const stored = localStorage.getItem(CACHE_KEY);
    if (stored) {
      const parsed = JSON.parse(stored);
      return new Map(Object.entries(parsed));
    }
  } catch (e) {
    console.error('Failed to load lipsync cache:', e);
  }
  return new Map();
}

// ä¿å­˜ç¼“å­˜åˆ° localStorage
function saveCache(cache: Map<string, LipsyncCacheEntry>) {
  try {
    const obj: Record<string, LipsyncCacheEntry> = {};
    cache.forEach((value, key) => {
      obj[key] = value;
    });
    localStorage.setItem(CACHE_KEY, JSON.stringify(obj));
  } catch (e) {
    console.error('Failed to save lipsync cache:', e);
  }
}

// æ¸…ç†è¿‡æœŸç¼“å­˜
function cleanExpiredCache(cache: Map<string, LipsyncCacheEntry>): Map<string, LipsyncCacheEntry> {
  const now = Date.now();
  const entries = Array.from(cache.entries());
  
  // è¿‡æ»¤æ‰è¿‡æœŸçš„æ¡ç›®
  const valid = entries.filter(([, entry]) => now - entry.createdAt < CACHE_EXPIRY_MS);
  
  // å¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼Œåˆ é™¤æœ€æ—§çš„
  if (valid.length > MAX_CACHE_ENTRIES) {
    valid.sort((a, b) => b[1].createdAt - a[1].createdAt);
    return new Map(valid.slice(0, MAX_CACHE_ENTRIES));
  }
  
  return new Map(valid);
}

// åºåˆ—åŒ–æ¶ˆæ¯ç”¨äºå­˜å‚¨
function serializeMessages(messages: Message[]): string {
  return JSON.stringify(messages.map(m => ({
    ...m,
    timestamp: m.timestamp.toISOString(),
  })));
}

// ååºåˆ—åŒ–å­˜å‚¨çš„æ¶ˆæ¯
function deserializeMessages(data: string): Message[] {
  try {
    const parsed = JSON.parse(data);
    return parsed.map((m: { id: string; role: 'user' | 'assistant'; content: string; timestamp: string }) => ({
      ...m,
      timestamp: new Date(m.timestamp),
    }));
  } catch {
    return [];
  }
}

// ä» localStorage åŠ è½½èŠå¤©è®°å½•
function loadStoredMessages(): Message[] {
  try {
    const stored = localStorage.getItem(CHAT_HISTORY_KEY);
    if (stored) {
      console.log('Loaded chat history from localStorage');
      return deserializeMessages(stored);
    }
  } catch (e) {
    console.error('Failed to load chat history:', e);
  }
  return [];
}

interface UseVideoCallOptions {
  settings: AppSettings;
  systemPrompt: string;
  onSpeakingChange?: (isSpeaking: boolean) => void;
  onLipsyncVideoReady?: (videoUrl: string) => void;
  onPresetAnimationTrigger?: (audioBase64: string) => void; // ä¼ é€’éŸ³é¢‘æ•°æ®ç”¨äºåŒæ­¥
}

export function useVideoCall({ settings, systemPrompt, onSpeakingChange, onLipsyncVideoReady, onPresetAnimationTrigger }: UseVideoCallOptions) {
  const [messages, setMessages] = useState<Message[]>(() => loadStoredMessages());
  const [isLoading, setIsLoading] = useState(false);
  const [isCameraActive, setIsCameraActive] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [isGeneratingLipsync, setIsGeneratingLipsync] = useState(false);
  const [interimTranscript, setInterimTranscript] = useState('');
  
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const lipsyncCacheRef = useRef<Map<string, LipsyncCacheEntry>>(loadCache());
  const pendingTranscriptRef = useRef<string>('');

  // ä½¿ç”¨è®°å¿†ç®¡ç†å™¨
  const {
    memorySummary,
    isSummarizing,
    checkAndSummarize,
    buildContextMessages,
    clearMemory,
    updateMemorySummary,
  } = useMemoryManager();

  // ç”¨ ref ä¿å­˜ sendMessageï¼Œé¿å… STT å›è°ƒé—­åŒ…è¿‡æœŸ
  const sendMessageRef = useRef<(content: string, includeImage?: boolean) => Promise<void>>(null as any);

  // STT å›è°ƒ
  const handleSTTResult = useCallback((transcript: string, isFinal: boolean) => {
    if (isFinal) {
      // æœ€ç»ˆç»“æœï¼Œå‘é€æ¶ˆæ¯
      if (transcript.trim()) {
        sendMessageRef.current?.(transcript.trim(), true);
      }
      setInterimTranscript('');
      pendingTranscriptRef.current = '';
    } else {
      // ä¸´æ—¶ç»“æœï¼Œä»…æ˜¾ç¤º
      setInterimTranscript(transcript);
      pendingTranscriptRef.current = transcript;
    }
  }, []);

  const handleSTTError = useCallback((error: string) => {
    console.error('[STT Error]', error);
    setInterimTranscript('');
    pendingTranscriptRef.current = '';
  }, []);

  // å°è¯•ä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«
  const xunfeiSTT = useXunfeiSTT({
    language: 'zh_cn',
    onResult: handleSTTResult,
    onError: handleSTTError,
  });

  // å¤‡ç”¨ï¼šWeb Speech API
  const webSpeechSTT = useWebSpeechSTT({
    language: 'zh-CN',
    onResult: handleSTTResult,
    onError: handleSTTError,
  });

  // ä¼˜å…ˆä½¿ç”¨è®¯é£ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ Web Speech API
  const sttEngine = xunfeiSTT.isSupported ? xunfeiSTT : webSpeechSTT;
  const {
    isListening: isRecording,
    interimTranscript: sttInterim,
    startListening,
    stopListening,
  } = sttEngine;

  console.log('[STT] Using engine:', xunfeiSTT.isSupported ? 'Xunfei' : 'Web Speech API');

  // åŒæ­¥ STT çš„ä¸´æ—¶è¯†åˆ«ç»“æœ
  useEffect(() => {
    setInterimTranscript(sttInterim);
  }, [sttInterim]);

  const isProcessingVoice = false; // Web Speech API ä¸éœ€è¦å¤„ç†å»¶è¿Ÿ

  // ä¿å­˜èŠå¤©è®°å½•åˆ° localStorage
  useEffect(() => {
    if (messages.length > 0) {
      try {
        const messagesToStore = messages.slice(-MAX_STORED_MESSAGES);
        localStorage.setItem(CHAT_HISTORY_KEY, serializeMessages(messagesToStore));
      } catch (e) {
        console.error('Failed to save chat history:', e);
      }
    }
  }, [messages]);

  // é€šçŸ¥çˆ¶ç»„ä»¶è¯´è¯çŠ¶æ€å˜åŒ–
  useEffect(() => {
    onSpeakingChange?.(isPlaying);
  }, [isPlaying, onSpeakingChange]);

  // å¯åŠ¨æ‘„åƒå¤´
  const startCamera = useCallback(async (videoElement: HTMLVideoElement, facing: 'user' | 'environment' = 'user') => {
    try {
      // å…ˆåœæ‰æ—§çš„æµ
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: facing,
        },
        audio: false,
      });
      
      videoElement.srcObject = stream;
      await videoElement.play();
      
      videoRef.current = videoElement;
      streamRef.current = stream;
      setIsCameraActive(true);
      
      // åˆ›å»ºç”¨äºæˆªå›¾çš„canvas
      const canvas = document.createElement('canvas');
      canvas.width = 640;
      canvas.height = 480;
      canvasRef.current = canvas;
      
      return true;
    } catch (error) {
      console.error('Failed to start camera:', error);
      return false;
    }
  }, []);

  // å…³é—­æ‘„åƒå¤´
  const stopCamera = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
      videoRef.current = null;
    }
    setIsCameraActive(false);
  }, []);

  // æˆªå–å½“å‰ç”»é¢
  const captureFrame = useCallback((): string | null => {
    const video = videoRef.current;
    const canvas = canvasRef.current;
    if (!video || !canvas) return null;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return null;
    
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    return canvas.toDataURL('image/jpeg', 0.8);
  }, []);

  // å¼€å§‹å½•éŸ³
  const startRecording = useCallback(async () => {
    try {
      await startListening();
    } catch (e) {
      throw new Error('Failed to start speech recognition');
    }
  }, [startListening]);

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(async (): Promise<string> => {
    stopListening();
    // Web Speech API ä¼šé€šè¿‡ onResult å›è°ƒè¿”å›ç»“æœ
    // è¿™é‡Œè¿”å›å½“å‰çš„ä¸´æ—¶è¯†åˆ«ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    const result = pendingTranscriptRef.current || '';
    pendingTranscriptRef.current = '';
    return result;
  }, [stopListening]);

  // å°†è§’è‰²å›¾ç‰‡è½¬æ¢ä¸º base64 data URL (fal.ai å¯ä»¥ç›´æ¥ä½¿ç”¨)
  const getCharacterImageDataUrl = useCallback(async (): Promise<string> => {
    try {
      // Fetch the image and convert to base64
      const response = await fetch(characterFrontImg);
      const blob = await response.blob();
      
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onloadend = () => {
          const dataUrl = reader.result as string;
          console.log('Character image converted to data URL, length:', dataUrl.length);
          resolve(dataUrl);
        };
        reader.onerror = reject;
        reader.readAsDataURL(blob);
      });
    } catch (error) {
      console.error('Failed to convert image to data URL:', error);
      // Fallback to the original URL
      const baseUrl = window.location.origin;
      return `${baseUrl}${characterFrontImg}`;
    }
  }, []);

  // æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰è§†é¢‘
  const getCachedVideo = useCallback((text: string): LipsyncCacheEntry | null => {
    const hash = hashText(text);
    const cache = cleanExpiredCache(lipsyncCacheRef.current);
    lipsyncCacheRef.current = cache;
    saveCache(cache);
    
    const entry = cache.get(hash);
    if (entry && Date.now() - entry.createdAt < CACHE_EXPIRY_MS) {
      console.log('Found cached lipsync video for text hash:', hash);
      return entry;
    }
    return null;
  }, []);

  // ä¿å­˜è§†é¢‘åˆ°ç¼“å­˜
  const cacheVideo = useCallback((text: string, videoUrl: string, audioBase64: string) => {
    const hash = hashText(text);
    const cache = cleanExpiredCache(lipsyncCacheRef.current);
    
    cache.set(hash, {
      videoUrl,
      audioBase64,
      createdAt: Date.now(),
    });
    
    lipsyncCacheRef.current = cache;
    saveCache(cache);
    console.log('Cached lipsync video for text hash:', hash);
  }, []);

  // ç”Ÿæˆå”‡å½¢åŠ¨ç”»è§†é¢‘ - æ”¯æŒå¤šå¼•æ“
  const generateLipsyncVideo = useCallback(async (audioBase64: string, text: string): Promise<string | null> => {
    try {
      setIsGeneratingLipsync(true);
      const engine = settings.voiceConfig.lipsyncEngine || 'musetalk';
      console.log(`Generating lipsync video with engine: ${engine}`);
      
      const imageUrl = await getCharacterImageDataUrl();
      console.log('Character image data URL length:', imageUrl.length);

      // æ ¹æ®é€‰æ‹©çš„å¼•æ“è°ƒç”¨ä¸åŒçš„ API
      let functionName: string;
      let requestBody: Record<string, unknown>;

      if (engine === 'musetalk') {
        functionName = 'musetalk-lipsync';
        requestBody = {
          imageUrl,
          audioBase64,
        };
      } else {
        functionName = 'omnihuman-lipsync';
        requestBody = {
          imageUrl,
          audioBase64,
          resolution: '720p',
          turboMode: true,
        };
      }

      console.log(`Calling ${functionName}...`);
      const { data, error } = await supabase.functions.invoke(functionName, {
        body: requestBody,
      });

      if (error) {
        console.error(`${engine} function error:`, error);
        
        // å¦‚æœä¸»å¼•æ“å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¼•æ“
        if (engine === 'musetalk') {
          console.log('MuseTalk failed, falling back to OmniHuman...');
          const fallbackResult = await supabase.functions.invoke('omnihuman-lipsync', {
            body: {
              imageUrl,
              audioBase64,
              resolution: '720p',
              turboMode: true,
            },
          });
          
          if (fallbackResult.data?.videoUrl) {
            console.log('Fallback to OmniHuman succeeded:', fallbackResult.data.videoUrl);
            cacheVideo(text, fallbackResult.data.videoUrl, audioBase64);
            return fallbackResult.data.videoUrl;
          }
        }
        return null;
      }

      if (data?.error) {
        console.error(`${engine} API error:`, data.error);
        return null;
      }

      if (data?.videoUrl) {
        console.log('Lipsync video generated:', data.videoUrl);
        cacheVideo(text, data.videoUrl, audioBase64);
        return data.videoUrl;
      }

      return null;
    } catch (error) {
      console.error('Lipsync generation error:', error);
      return null;
    } finally {
      setIsGeneratingLipsync(false);
    }
  }, [getCharacterImageDataUrl, cacheVideo, settings.voiceConfig.lipsyncEngine]);

  // åŒæ­¥æ’­æ”¾éŸ³é¢‘å’Œè§†é¢‘
  const playSynced = useCallback(async (audioBase64: string, videoUrl: string) => {
    const audioUrl = `data:audio/mpeg;base64,${audioBase64}`;
    const audio = new Audio(audioUrl);
    audioRef.current = audio;
    
    // å…ˆé€šçŸ¥è§†é¢‘å‡†å¤‡å¥½
    if (onLipsyncVideoReady) {
      onLipsyncVideoReady(videoUrl);
    }
    
    // çŸ­æš‚å»¶è¿Ÿè®©è§†é¢‘å…ƒç´ åŠ è½½
    await new Promise(resolve => setTimeout(resolve, 100));
    
    audio.onplay = () => setIsPlaying(true);
    audio.onended = () => setIsPlaying(false);
    audio.onerror = () => setIsPlaying(false);
    
    await audio.play();
  }, [onLipsyncVideoReady]);

  // TTS æ’­æ”¾ï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©é¢„è®¾åŠ¨ç”»æˆ–ç”ŸæˆåŠ¨ç”»ï¼‰
  const speak = useCallback(async (text: string) => {
    const { voiceConfig } = settings;
    if (!voiceConfig.enabled || !voiceConfig.minimaxApiKey || !voiceConfig.minimaxGroupId) {
      console.log('Voice not enabled or missing config');
      return;
    }

    // ç§»é™¤æ‹¬å·å†…çš„å†…å®¹ï¼Œä¸æœ—è¯»
    const textToSpeak = removeParenthesesContent(text);
    if (!textToSpeak) {
      console.log('No text to speak after removing parentheses content');
      return;
    }

    const lipsyncMode = voiceConfig.lipsyncMode || 'preset';

    // é¢„è®¾åŠ¨ç”»æ¨¡å¼ï¼šç”ŸæˆTTSåç”±é¢„è®¾åŠ¨ç”»ç³»ç»ŸåŒæ­¥æ’­æ”¾
    if (lipsyncMode === 'preset') {
      try {
        console.log('Generating TTS audio (preset mode)...');
        import('sonner').then(({ toast }) => toast.info(`ğŸ¤ è°ƒç”¨MiniMax TTS...`));
        const response = await fetch(
          `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/minimax-tts`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY}`,
            },
            body: JSON.stringify({
              text: textToSpeak,
              apiKey: voiceConfig.minimaxApiKey,
              groupId: voiceConfig.minimaxGroupId,
              voiceId: voiceConfig.voiceId,
            }),
          }
        );

        if (!response.ok) {
          const error = await response.json();
          import('sonner').then(({ toast }) => toast.error(`âŒ TTS APIå¤±è´¥: ${error.error || response.status}`));
          throw new Error(error.error || 'TTS request failed');
        }

        const data = await response.json();
        
        if (data.audioContent) {
          console.log('TTS audio ready, passing to preset animation system for synced playback...');
          import('sonner').then(({ toast }) => toast.success(`âœ… éŸ³é¢‘å°±ç»ª, é•¿åº¦=${data.audioContent.length}, æœ‰åŠ¨ç”»å›è°ƒ=${!!onPresetAnimationTrigger}`));
          
          // å°†éŸ³é¢‘æ•°æ®ä¼ é€’ç»™é¢„è®¾åŠ¨ç”»ç³»ç»Ÿï¼Œç”±å®ƒæ¥å¤„ç†åŒæ­¥æ’­æ”¾
          if (onPresetAnimationTrigger) {
            await onPresetAnimationTrigger(data.audioContent);
          } else {
            // åå¤‡ï¼šå¦‚æœæ²¡æœ‰é¢„è®¾åŠ¨ç”»å¤„ç†å™¨ï¼Œç›´æ¥æ’­æ”¾éŸ³é¢‘
            const audioUrl = `data:audio/mpeg;base64,${data.audioContent}`;
            const audio = new Audio(audioUrl);
            audioRef.current = audio;
            audio.onplay = () => setIsPlaying(true);
            audio.onended = () => setIsPlaying(false);
            audio.onerror = () => setIsPlaying(false);
            await audio.play();
          }
        } else {
          import('sonner').then(({ toast }) => toast.error(`âŒ TTSè¿”å›æ— éŸ³é¢‘æ•°æ®`));
        }
      } catch (error) {
        console.error('TTS error:', error);
        import('sonner').then(({ toast }) => toast.error(`âŒ TTSå¼‚å¸¸: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`));
      }
      return;
    }

    // AIç”Ÿæˆæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
    // å…ˆæ£€æŸ¥ç¼“å­˜ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„æ–‡æœ¬ä½œä¸ºkeyï¼‰
    const cached = getCachedVideo(textToSpeak);
    if (cached) {
      console.log('Using cached lipsync video - playing synced');
      await playSynced(cached.audioBase64, cached.videoUrl);
      return;
    }

    try {
      // Step 1: ç”Ÿæˆ TTS éŸ³é¢‘
      console.log('Generating TTS audio...');
      const response = await fetch(
        `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/minimax-tts`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY}`,
          },
          body: JSON.stringify({
            text: textToSpeak,
            apiKey: voiceConfig.minimaxApiKey,
            groupId: voiceConfig.minimaxGroupId,
            voiceId: voiceConfig.voiceId,
          }),
        }
      );

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || 'TTS request failed');
      }

      const data = await response.json();
      
      if (data.audioContent) {
        // Step 2: ç­‰å¾…è§†é¢‘ç”Ÿæˆå®Œæˆ
        console.log('TTS audio ready, generating lipsync video...');
        const videoUrl = await generateLipsyncVideo(data.audioContent, textToSpeak);
        
        if (videoUrl) {
          // Step 3: è§†é¢‘ç”Ÿæˆå®Œæˆï¼ŒåŒæ­¥æ’­æ”¾éŸ³é¢‘å’Œè§†é¢‘
          console.log('Video ready, playing synced audio and video');
          await playSynced(data.audioContent, videoUrl);
        } else {
          // è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œä»…æ’­æ”¾éŸ³é¢‘
          console.log('Video generation failed, playing audio only');
          const audioUrl = `data:audio/mpeg;base64,${data.audioContent}`;
          const audio = new Audio(audioUrl);
          audioRef.current = audio;
          
          audio.onplay = () => setIsPlaying(true);
          audio.onended = () => setIsPlaying(false);
          audio.onerror = () => setIsPlaying(false);
          
          await audio.play();
        }
      }
    } catch (error) {
      console.error('TTS error:', error);
    }
  }, [settings, generateLipsyncVideo, getCachedVideo, playSynced, onPresetAnimationTrigger]);

  // åœæ­¢æ’­æ”¾
  const stopPlaying = useCallback(() => {
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
      setIsPlaying(false);
    }
  }, []);

  // å‘é€æ¶ˆæ¯ï¼ˆå¸¦è§†è§‰ï¼‰
  const sendMessage = useCallback(async (content: string, includeImage = true) => {
    const userMessage: Message = {
      id: crypto.randomUUID(),
      role: 'user',
      content,
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setIsLoading(true);

    try {
      // æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“æ—§å¯¹è¯
      const allMessages = [...messages, userMessage];
      await checkAndSummarize(
        allMessages,
        import.meta.env.VITE_SUPABASE_URL,
        import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY
      );

      // æ„å»ºä¸Šä¸‹æ–‡æ¶ˆæ¯ï¼ˆåŒ…å«è®°å¿†æ‘˜è¦ + æœ€è¿‘æ¶ˆæ¯ï¼‰
      const contextMessages = buildContextMessages(allMessages);

      // æˆªå–å½“å‰ç”»é¢
      const image = includeImage ? captureFrame() : null;
      
      // æ¯æ¬¡å‘æ¶ˆæ¯æ—¶å®æ—¶æ³¨å…¥å½“å‰æ—¶é—´
      const nowDate = new Date();
      const nowStr = nowDate.toLocaleString('zh-CN', {
        timeZone: 'Asia/Shanghai',
        year: 'numeric',
        month: 'long',
        day: 'numeric',
        weekday: 'long',
        hour: '2-digit',
        minute: '2-digit',
      });
      const cnHour = parseInt(nowDate.toLocaleString('zh-CN', { timeZone: 'Asia/Shanghai', hour: 'numeric', hour12: false }));
      const period = cnHour < 6 ? 'å‡Œæ™¨' : cnHour < 9 ? 'æ—©ä¸Š' : cnHour < 12 ? 'ä¸Šåˆ' : cnHour < 14 ? 'ä¸­åˆ' : cnHour < 18 ? 'ä¸‹åˆ' : cnHour < 22 ? 'æ™šä¸Š' : 'æ·±å¤œ';
      const realtimePrompt = systemPrompt.replace(
        /å½“å‰æ—¶é—´ï¼š.*/,
        `å½“å‰æ—¶é—´ï¼š${nowStr}ï¼ˆ${period}ï¼‰`
      );

      const response = await fetch(
        `${import.meta.env.VITE_SUPABASE_URL}/functions/v1/vision-chat`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${import.meta.env.VITE_SUPABASE_PUBLISHABLE_KEY}`,
          },
          body: JSON.stringify({
            messages: contextMessages,
            systemPrompt: realtimePrompt,
            image,
          }),
        }
      );

      if (!response.ok) {
        if (response.status === 429) {
          throw new Error('è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•');
        }
        if (response.status === 402) {
          throw new Error('APIé¢åº¦å·²ç”¨å®Œï¼Œè¯·å……å€¼');
        }
        throw new Error('AIå›å¤å¤±è´¥');
      }

      const reader = response.body?.getReader();
      if (!reader) throw new Error('æ— æ³•è¯»å–å“åº”');

      const decoder = new TextDecoder();
      let assistantContent = '';
      const assistantMessageId = crypto.randomUUID();

      setMessages(prev => [
        ...prev,
        {
          id: assistantMessageId,
          role: 'assistant',
          content: '',
          timestamp: new Date(),
        },
      ]);

      let textBuffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        textBuffer += decoder.decode(value, { stream: true });

        let newlineIndex: number;
        while ((newlineIndex = textBuffer.indexOf('\n')) !== -1) {
          let line = textBuffer.slice(0, newlineIndex);
          textBuffer = textBuffer.slice(newlineIndex + 1);

          if (line.endsWith('\r')) line = line.slice(0, -1);
          if (line.startsWith(':') || line.trim() === '') continue;
          if (!line.startsWith('data: ')) continue;

          const jsonStr = line.slice(6).trim();
          if (jsonStr === '[DONE]') break;

          try {
            const parsed = JSON.parse(jsonStr);
            const deltaContent = parsed.choices?.[0]?.delta?.content as string | undefined;
            if (deltaContent) {
              assistantContent += deltaContent;
              setMessages(prev =>
                prev.map(m =>
                  m.id === assistantMessageId
                    ? { ...m, content: assistantContent }
                    : m
                )
              );
            }
          } catch {
            textBuffer = line + '\n' + textBuffer;
            break;
          }
        }
      }

      // è‡ªåŠ¨æ’­æ”¾TTS
      if (assistantContent && settings.voiceConfig.enabled) {
        import('sonner').then(({ toast }) => toast.info(`ğŸ”Š TTSå¼€å§‹: "${assistantContent.substring(0, 20)}..."`));
        await speak(assistantContent);
      } else {
        import('sonner').then(({ toast }) => toast.warning(`âš ï¸ TTSè·³è¿‡: å†…å®¹=${!!assistantContent}, è¯­éŸ³=${settings.voiceConfig.enabled}`));
      }

      return assistantContent;
    } catch (error) {
      console.error('Video call error:', error);
      const errorMessage: Message = {
        id: crypto.randomUUID(),
        role: 'assistant',
        content: error instanceof Error ? error.message : 'æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, errorMessage]);
      return null;
    } finally {
      setIsLoading(false);
    }
  }, [messages, systemPrompt, captureFrame, settings, speak, checkAndSummarize, buildContextMessages]);

  // åŒæ­¥ sendMessage åˆ° refï¼Œè®© STT å›è°ƒå§‹ç»ˆè°ƒç”¨æœ€æ–°ç‰ˆæœ¬
  useEffect(() => {
    sendMessageRef.current = sendMessage;
  }, [sendMessage]);

  // æ¸…é™¤æ¶ˆæ¯
  const clearMessages = useCallback(() => {
    setMessages([]);
    try {
      localStorage.removeItem(CHAT_HISTORY_KEY);
    } catch (e) {
      console.error('Failed to clear chat history:', e);
    }
  }, []);

  // åˆ é™¤æŒ‡å®šæ¶ˆæ¯
  const deleteMessages = useCallback((messageIds: string[]) => {
    setMessages(prev => prev.filter(m => !messageIds.includes(m.id)));
  }, []);

  // ç¼–è¾‘æŒ‡å®šæ¶ˆæ¯
  const editMessage = useCallback((messageId: string, newContent: string) => {
    setMessages(prev => prev.map(m => 
      m.id === messageId ? { ...m, content: newContent } : m
    ));
  }, []);

  // æ¸…ç†
  useEffect(() => {
    return () => {
      stopCamera();
      stopPlaying();
    };
  }, [stopCamera, stopPlaying]);

  return {
    messages,
    isLoading,
    isCameraActive,
    isRecording,
    isProcessingVoice,
    isPlaying,
    isGeneratingLipsync,
    interimTranscript,
    memorySummary,
    isSummarizing,
    startCamera,
    stopCamera,
    captureFrame,
    startRecording,
    stopRecording,
    sendMessage,
    clearMessages,
    deleteMessages,
    editMessage,
    clearMemory,
    updateMemorySummary,
    speak,
    stopPlaying,
  };
}
